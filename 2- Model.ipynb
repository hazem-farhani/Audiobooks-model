{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Create the machine learning algorithm__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Import the relevant libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "# We extract the inputs using the keyword under which we saved them.\n",
    "train_inputs = npz['inputs'].astype(np.float)  # to ensure that they are all floats\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them).\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model__\n",
    "Outline, optimizers, loss, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 0s - loss: 0.6266 - accuracy: 0.6527 - val_loss: 0.5333 - val_accuracy: 0.7271\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.4697 - accuracy: 0.7734 - val_loss: 0.4435 - val_accuracy: 0.7606\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.4141 - accuracy: 0.7837 - val_loss: 0.4113 - val_accuracy: 0.7875\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3878 - accuracy: 0.8041 - val_loss: 0.3914 - val_accuracy: 0.7987\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3711 - accuracy: 0.8002 - val_loss: 0.3800 - val_accuracy: 0.8076\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.3620 - accuracy: 0.8125 - val_loss: 0.3789 - val_accuracy: 0.7942\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.3563 - accuracy: 0.8131 - val_loss: 0.3662 - val_accuracy: 0.8188\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.3520 - accuracy: 0.8145 - val_loss: 0.3636 - val_accuracy: 0.8098\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.3457 - accuracy: 0.8153 - val_loss: 0.3601 - val_accuracy: 0.8076\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.3466 - accuracy: 0.8033 - val_loss: 0.3553 - val_accuracy: 0.8054\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.3459 - accuracy: 0.8136 - val_loss: 0.3550 - val_accuracy: 0.8188\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.3448 - accuracy: 0.8094 - val_loss: 0.3532 - val_accuracy: 0.8166\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.3350 - accuracy: 0.8226 - val_loss: 0.3537 - val_accuracy: 0.8009\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.3355 - accuracy: 0.8220 - val_loss: 0.3588 - val_accuracy: 0.7875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c4dbd35948>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50   # Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),  # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')  # output layer\n",
    "])\n",
    "\n",
    "\n",
    "### Choose the optimizer and the loss function\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "### Training\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "# set an early stopping mechanism, let's set patience=2, to be a bit tolerant against random validation loss increases.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets,\n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          verbose = 2\n",
    "          )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Test the model__\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset. \n",
    "\n",
    "The test is the absolute final instance. You should not test before you are completely done with adjusting your model.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 257us/sample - loss: 0.3220 - accuracy: 0.8125\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.32. Test accuracy: 81.25%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
